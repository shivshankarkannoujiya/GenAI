{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00303b29",
   "metadata": {},
   "source": [
    "**SIMPLE RAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb10754f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = Path.cwd() / \"nodejs.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e0e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(file_path=pdf_path)\n",
    "docs = loader.load()  # docs: list[Document] <By default pages bna kr de dega>\n",
    "# print(docs[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4207e23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take text_Splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bcb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 200\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents=docs)\n",
    "print(f\"DOCS: {len(docs)}\")\n",
    "print(f\"SPLIT_DOCS: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4aec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bf8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0034dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want to do embeddings of split_docs and save in vector DB\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "\n",
    "# qdrant_vector_store = QdrantVectorStore.from_documents(\n",
    "#     documents=[],\n",
    "#     url=\"http://localhost:6333\",\n",
    "#     collection_name=\"learning_rag_with_langchain\",\n",
    "#     embedding=embedder\n",
    "# )\n",
    "\n",
    "# qdrant_vector_store.add_documents(documents=split_docs)\n",
    "# print(f\"---------- INJECTION DONE ----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e9afae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETRIVING FROM SAME DB\n",
    "# We should create it in another file\n",
    "\n",
    "retriver = QdrantVectorStore.from_existing_collection(\n",
    "    url=\"http://localhost:6333\",\n",
    "    collection_name=\"learning_rag_with_langchain\",\n",
    "    embedding=embedder\n",
    ")\n",
    "\n",
    "# RELEVANT CHUNKS\n",
    "# relevant_chunks = retriver.similarity_search(\n",
    "#     query=\"What is FS Module?\"\n",
    "# )\n",
    "\n",
    "# print(f\"RELEVENT CHUNKS: {relevant_chunks}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7119e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    api_key=OPENAI_API_KEY    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa273e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: BASED ON THE RELEVENT CHUNKS WE CAN CHAT WITH THE LLM\n",
    "# GIVE THE RELEVENT CHUNKS INTO THE CONTEXT\n",
    "\n",
    "# SYSTEM_PROMPT = f\"\"\"\n",
    "# You are a helpfull AI Assistant who give response of the user query based on the available CONTEXT\n",
    "\n",
    "# CONTEXT\n",
    "# {relevant_chunks}\n",
    "\n",
    "# Provide a detailed answer based only on the context. If you cannot answer from the context, say so.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3989f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Chatbot is ready! (Type 'exit' to stop)\")\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"Ask: \")\n",
    "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "\n",
    "    relevant_chunks = retriver.similarity_search(\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    # 4. Construct System Prompt with Context\n",
    "    context_text = \"\\n\".join([doc.page_content for doc in relevant_chunks])\n",
    "    system_prompt_content = f\"\"\"\n",
    "    You are a helpful AI Assistant who gives responses based on the available CONTEXT.\n",
    "    \n",
    "    CONTEXT:\n",
    "    {context_text}\n",
    "    \n",
    "    Provide a detailed answer based only on the context. If you cannot answer from the context, say so.\n",
    "    \"\"\"\n",
    "\n",
    "    # 5. Build Message List (History + New Query)\n",
    "    messages = [SystemMessage(content=system_prompt_content)]\n",
    "    messages.extend(chat_history)  # Add previous messages\n",
    "    messages.append(HumanMessage(content=user_query))  # Add current query\n",
    "\n",
    "    # get Response\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    # 7. Update History\n",
    "    chat_history.append(HumanMessage(content=user_query))\n",
    "    chat_history.append(AIMessage(content=response.content))\n",
    "\n",
    "    print(f\"AI: {response.content}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
